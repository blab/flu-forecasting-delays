---
title: "Model distances between simulated populations"
output: html_document
date: "2023-03-31"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/jhuddlesfredhutch.org/projects/flu-forecasting-delays")

library(dplyr)
library(readr)
library(ggplot2)
library(rethinking)
```

## Load and prepare data

Load distances between populations at a given current timepoint and a corresponding future timepoint.
The future timepoint corresponds to the forecast horizon (in months) used to make the forecast.

```{r}
distances <- read_csv("results/model_inputs.csv")
```

Standardize submission delay and forecast horizon values for use in models.
Both predictors use units of "months".

```{r}
distances$horizon_s <- (distances$horizon - mean(distances$horizon)) / sd(distances$horizon)
distances$delay_s <- (distances$delay - mean(distances$delay)) / sd(distances$delay)
```

## Run prior predictive checks

Determine priors for each model parameter such that the possible output is within realistic bounds.

```{r}
# Make combinations of horizons and delays.
horizons <- unique(distances$horizon)
delays <- unique(distances$delay)
horizons_delays <- expand.grid(delay=delays, horizon=horizons)

# Define total samples.
n_samples_per_input <- 5000
n_inputs <- length(horizons_delays$delay)
n_samples <- n_samples_per_input * n_inputs

# Make vectors of horizons and delays.
horizon <- rep(horizons_delays$horizon, n_samples_per_input)
delay <- rep(horizons_delays$delay, n_samples_per_input)
```

Sample from the appropriate distribution for each prior.

```{r}
# Sample scale values.
prior.scale <- rexp(n_samples, 1)

# Sample intercept and coefficients.
a <- rnorm(n_samples, 0, 0.075)
b_s <- rnorm(n_samples, 0, 0.075)
b_h <- rnorm(n_samples, 0, 0.075)

# Calculate the rate of the gamma distribution.
prior.mu <- exp(a + (b_s * delay) + (b_h * horizon))

# Sample distances to the future.
prior.distance <- rgamma2(n_samples, prior.mu, prior.scale)
```

Build a data frame of the prior results.

```{r}
priors <- data.frame(
  horizon=horizon,
  delay=delay,
  distance=prior.distance
)
```

Summarize prior distances by feature.

```{r}
priors %>% group_by(horizon, delay) %>% summarize(median=median(distance), mean=mean(distance))
```

Plot prior distributions.

```{r}
ggplot(priors, aes(as.factor(horizon), distance, colour=as.factor(delay))) + geom_boxplot() + theme_classic()
ggsave("prior-predictive-simulation.png")
```

## Fit a simple model

Fit a simple linear model with a single intercept to represent the average distance between populations across all samples when standardized delay and horizon values are zero.

```{r}
simple.model <- ulam(
  alist(
    distance ~ dgamma2(mu, scale),
    scale ~ dexp(1),
    log(mu) <- a + b_s * delay + b_h * horizon,
    a ~ dnorm(0, 0.075),
    b_s ~ dnorm(0, 0.075),
    b_h ~ dnorm(0, 0.075)
  ),
  data=distances,
  iter=10000,
  chains=4,
  cores=4,
  log_lik=TRUE
)
```

Summarize the model.

```{r}
precis(simple.model)
```

Plot the model coefficients and HPDI (?).

```{r}
plot(simple.model)
```

Most of the variation in model parameters appears in the "scale" parameter of the gamma distribution.

Just to see what happens, I tried plotting the relationship between different predictors and distance using the rethinking code examples from page 113.
In this example, I fixed the submission delay to the maximum value which is the "realistic" delay.

```{r}
horizon.seq <- unique(distances$horizon)
delay.seq <- rep(max(distances$delay), length(horizon.seq))
pred_dat <- list(horizon=horizon.seq, delay=delay.seq)
mu <- link(simple.model, data=pred_dat)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob=0.89)
sim.distance <- sim(simple.model, data=pred_dat)
distance.PI <- apply(sim.distance, 2, PI, prob=0.89)
```

Plot the results.

```{r}
sim.distance.df <- cbind(
  data.frame(
    horizon_start=3,
    distance_start=sim.distance[, 1]
  ),
  data.frame(
    horizon_end=12,
    distance_end=sim.distance[, 4]
  )
)
ggplot(sim.distance.df, aes(x=horizon_start, xend=horizon_end, y=distance_start, yend=distance_end)) + geom_segment(alpha=0.1) + theme_classic() + geom_point(aes(x=3, y=mu.mean[1], col="red")) + geom_point(aes(x=6, y=mu.mean[2], col="red")) + geom_point(aes(x=9, y=mu.mean[3], col="red")) + geom_point(aes(x=12, y=mu.mean[4], col="red")) + labs(x="Forecast horizon (months)", y="Distance to the future (AAs)") + theme(legend.position="none") + scale_x_continuous(breaks=horizon.seq)
ggsave("posterior-simulations-with-realistic-delay.png")
```

```{r}
mu.mean
```

```{r}
mu.PI
```

```{r}
distance.PI
```

We also see from the coefficients above that the two predictors have almost no effect on the distance output compared to the intercept and scale.

To figure out what that distribution looks like, we need to convert posterior samples into mu values for the gamma.
As a first pass, though, we could assume mu=1 and get something like this.

```{r}
hist(rgamma2(10000, mu.mean[1], coef(simple.model)["scale"]))
```

```{r}
hist(rgamma2(10000, mu.mean[4], coef(simple.model)["scale"]))
```

Next, I wanted to see the distributions of the predictor coefficients at a higher resolution than the precis plot above.

```{r}
prior.samples <- extract.prior(simple.model, n=10000)
post.samples <- extract.samples(simple.model)
```

Coefficient for submission delay.

```{r}
plot(density(post.samples$b_s), xlim=c(-0.1, 0.16))
lines(density(prior.samples$b_s), col="red")
abline(v=mean(post.samples$b_s), lty=2)
```

Coefficient for forecast horizon.

```{r}
plot(density(post.samples$b_h), xlim=c(-0.1, 0.18))
lines(density(prior.samples$b_h), col="red")
abline(v=mean(post.samples$b_h), lty=2)
```

Intercept representing average distance to the future across timepoints.

```{r}
plot(density(post.samples$a), xlim=c(-0.125, 1.5))
lines(density(prior.samples$a), col="red")
abline(v=mean(post.samples$a), lty=2)
```

Scale of gamma distribution representing the variance around the central rate.

```{r}
plot(density(post.samples$scale), xlim=c(-0.125, 1.75))
lines(density(prior.samples$scale), col="red")
abline(v=mean(post.samples$scale), lty=2)
```

### Posterior predictive checks

```{r}
horizons <- unique(distances$horizon)
delays <- unique(distances$delay)
horizons_delays <- expand.grid(delay=delays, horizon=horizons)
post <- sim(simple.model, horizons_delays)
```

Map treatments to posterior samples.
TODO: Do this a better way.

```{r}
all.post <- rbind(
  data.frame(horizon=3, delay=0, distance=post[,1]),
  data.frame(horizon=3, delay=1, distance=post[,2]),
  data.frame(horizon=3, delay=3, distance=post[,3]),
  data.frame(horizon=6, delay=0, distance=post[,4]),
  data.frame(horizon=6, delay=1, distance=post[,5]),
  data.frame(horizon=6, delay=3, distance=post[,6]),
  data.frame(horizon=9, delay=0, distance=post[,7]),
  data.frame(horizon=9, delay=1, distance=post[,8]),
  data.frame(horizon=9, delay=3, distance=post[,9]),
  data.frame(horizon=12, delay=0, distance=post[,10]),
  data.frame(horizon=12, delay=1, distance=post[,11]),
  data.frame(horizon=12, delay=3, distance=post[,12])
)
```

Plot the posterior distributions by treatment.

```{r}
ggplot(all.post, aes(as.factor(horizon), distance, colour=as.factor(delay))) + geom_boxplot() + theme_classic() + labs(x="Forecast horizon (months)", y="Distance to the future (AAs)", colour="Mean submission delay (months)") + theme(legend.position=c(0.25, 0.8))
ggsave("posterior-predictive-simulations-across-treatments.png")
```

Plot observed distances.

```{r}
ggplot(distances, aes(as.factor(horizon), distance, colour=as.factor(delay))) + geom_boxplot() + theme_classic() + labs(x="Forecast horizon (months)", y="Distance to the future (AAs)", colour="Mean submission delay (months)") + theme(legend.position=c(0.25, 0.8))
ggsave("observed-distances-for-simulations-across-treatments.png")
```

Summarize distances from observed data.

```{r, rows.print=20}
observed.median.distances <- distances %>% group_by(horizon, delay) %>% summarize(median=median(distance))
observed.median.distances
```

Then, summarize the corresponding distances from posterior predictive simulations.

```{r, rows.print=20}
post.median.distances <- all.post %>% group_by(horizon, delay) %>% summarize(median=median(distance))
post.median.distances
```

```{r}
observed.median.distances$type <- "observed"
post.median.distances$type <- "posterior"
all.median.distances <- rbind(observed.median.distances, post.median.distances)
```

Plot observed and median distances by factor.

```{r}
ggplot(all.median.distances, aes(as.factor(horizon), median, colour=as.factor(delay))) + geom_point(aes(shape=type)) + theme_classic() + labs(x="Forecast horizon (months)", y="Distance to the future (AAs)", colour="Mean submission delay (months)", shape="Type") + theme(legend.position=c(0.25, 0.7))
ggsave("observed-and-posterior-median-distances-for-simulations-across-treatments.png")
```

The simple model systematically underestimates the distance to the future for 6-month forecasts and overestimates the distance for 12-month forecasts.
I wonder whether allowing each timepoint to have its own intercept in a multilevel model would better account for this issue.

## Fit a multilevel model

Assigns an intercept to each timepoint with a shared mean, allowing the model to account for timepoint-specific variation.

```{r, include=FALSE}
full.model <- ulam(
  alist(
    distance ~ dgamma2(mu, scale),
    scale ~ dexp(1),
    log(mu) <- a + a_timepoint[t] + b_s * delay + b_h * horizon,
    a_timepoint[t] ~ dnorm(0, sigma),
    a ~ dnorm(0, 0.075),
    sigma ~ dexp(1),
    b_s ~ dnorm(0, 0.075),
    b_h ~ dnorm(0, 0.075)
  ),
  data=distances,
  iter=10000,
  chains=4,
  cores=4,
  log_lik=TRUE
)
```

Summarize top-level model.

```{r}
precis(full.model)
```

Plot primary model parameters.

```{r}
plot(full.model)
```

```{r}
full.post.samples <- extract.samples(full.model)
```

```{r}
dens(full.post.samples$sigma, xlim=c(-0.25, 3), ylim=c(0, 25))
dens(full.post.samples$a, add=TRUE)
```

```{r}
dens(full.post.samples$b_s, col="blue", ylim=c(0, 300), xlim=c(0, 0.1))
dens(full.post.samples$b_h, add=TRUE, col="orange")
```

Summarize complete model.

```{r}
precis(full.model, depth=2)
```

Plot all model parameters.

```{r fig.width=6, fig.height=10}
plot(full.model, depth=2)
```

Plot variation of timepoint intercepts in temporal order.

```{r}
plot(coef(full.model)[2:97], type="l")
```

```{r}
hist(coef(full.model)[2:97])
```

```{r}
compare(simple.model, full.model)
```

### Posterior predictive checks

```{r}
post <- sim(full.model, horizons_delays)
```

Map treatments to posterior samples.
TODO: Do this a better way.

```{r}
all.post <- rbind(
  data.frame(horizon=6, delay=0, distance=post[,1]),
  data.frame(horizon=6, delay=1, distance=post[,2]),
  data.frame(horizon=6, delay=3, distance=post[,3]),
  data.frame(horizon=12, delay=0, distance=post[,4]),
  data.frame(horizon=12, delay=1, distance=post[,5]),
  data.frame(horizon=12, delay=3, distance=post[,6])
)
```

Plot the posterior distributions by treatment.

```{r}
ggplot(all.post, aes(as.factor(horizon), distance, colour=as.factor(delay))) + geom_boxplot() + theme_classic() + labs(x="Forecast horizon (months)", y="Distance to the future (AAs)", colour="Mean submission delay (months)") + theme(legend.position=c(0.25, 0.8))
ggsave("posterior-predictive-simulations-across-treatments.png")
```

Summarize distances from observed data.

```{r}
distances %>% group_by(horizon, delay) %>% summarize(median=median(distance))
```
Then, summarize the corresponding distances from posterior predictive simulations.

```{r}
all.post %>% group_by(horizon, delay) %>% summarize(median=median(distance))
```
The simple model systematically underestimates the distance to the future for 6-month forecasts and overestimates the distance for 12-month forecasts.
I wonder whether allowing each timepoint to have its own intercept in a multilevel model would better account for this issue.

## Fit a multilevel model with a Gaussian process

The high variance of the variance parameter for time-specific intercepts in the full model and relatively high degree of overfitting by that model compared to the simple model suggested that something we wrong with the full model.
Reading more about these models, I realized that my varying intercept variable of time is a continuous variable with correlated values.
As such, I should probably use a Gaussian process (GP) to represent the covariance between timepoints.

Read in the pairwise distances between timepoints.

```{r}
distances.between.years <- as.matrix(read.csv("results/distance_in_years.csv"))
```

Fit the model with a GP.

```{r}
gp.model <- ulam(
  alist(
    distance ~ dgamma2(mu, scale),
    scale ~ dexp(1),
    log(mu) <- a + a_timepoint[t] + b_s * delay + b_h * horizon,
    vector[96]: a_timepoint ~ multi_normal( 0 , SIGMAT ),
    matrix[96,96]: SIGMAT <- cov_GPL2( Dmat , etasq , rhosq , sigmasq ),
    etasq ~ dexp(1),
    rhosq ~ dexp(1),
    sigmasq ~ dexp(1),
    a ~ dnorm(0, 0.075),
    b_s ~ dnorm(0, 0.075),
    b_h ~ dnorm(0, 0.075)
  ),
  data=list(
    t=distances$t,
    delay=distances$delay,
    horizon=distances$horizon,
    Dmat=distances.between.years,
    distance=distances$distance
  ),
  iter=10000,
  chains=4,
  cores=4,
  log_lik=TRUE
)
```

```{r}
precis(gp.model)
```

```{r}
plot(precis(gp.model))
```

```{r}
compare(simple.model, full.model, gp.model)
```

